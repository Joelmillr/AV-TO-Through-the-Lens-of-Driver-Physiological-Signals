{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional Autoencoder (CAE) architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "        self.ls = 32\n",
    "\n",
    "        # Define separate encoders for each signal\n",
    "        self.ecg_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=20, stride=10, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, self.ls, kernel_size=10, stride=5, padding=1),\n",
    "        )\n",
    "        self.rsp_encoder = self.ecg_encoder\n",
    "        self.eda_tonic_encoder = self.ecg_encoder\n",
    "        self.eda_phasic_encoder = self.ecg_encoder\n",
    "\n",
    "        # Fully connected layer to compress the latent space\n",
    "        self.fc = nn.Linear(128, self.ls)\n",
    "\n",
    "        # Fully connected layer to decompress the latent space\n",
    "        self.fc_decoded = nn.Linear(self.ls, 128)\n",
    "\n",
    "        # Define separate decoders for each signal\n",
    "        self.ecg_decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(self.ls, 8, kernel_size=10, stride=5, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=20, stride=10, padding=1, output_padding=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.rsp_decoder = self.ecg_decoder\n",
    "        self.eda_tonic_decoder = self.ecg_decoder\n",
    "        self.eda_phasic_decoder = self.ecg_decoder\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Split input by channel for independent processing\n",
    "        ecg = x[:, 0, :].unsqueeze(1)\n",
    "        rsp = x[:, 1, :].unsqueeze(1)\n",
    "        eda_tonic = x[:, 2, :].unsqueeze(1)\n",
    "        eda_phasic = x[:, 3, :].unsqueeze(1)\n",
    "\n",
    "        # Encode each signal independently\n",
    "        ecg_encoded = self.ecg_encoder(ecg)\n",
    "        rsp_encoded = self.rsp_encoder(rsp)\n",
    "        eda_tonic_encoded = self.eda_tonic_encoder(eda_tonic)\n",
    "        eda_phasic_encoded = self.eda_phasic_encoder(eda_phasic)\n",
    "\n",
    "        # Concatenate the latent representations along the last dimension\n",
    "        latent_space = torch.cat(\n",
    "            (ecg_encoded, rsp_encoded, eda_tonic_encoded, eda_phasic_encoded), dim=1\n",
    "        )\n",
    "\n",
    "        latent_space = latent_space.permute(0, 2, 1)\n",
    "\n",
    "        # Compress the latent space\n",
    "        latent_space = self.fc(latent_space)\n",
    "\n",
    "        return latent_space\n",
    "\n",
    "    def decode(self, latent_space):\n",
    "        # Decompress the latent space\n",
    "        latent_space = self.fc_decoded(latent_space)\n",
    "        latent_space = latent_space.permute(0, 2, 1)\n",
    "\n",
    "        # Split latent space back into separate channels\n",
    "        ecg_latent, rsp_latent, eda_tonic_latent, eda_phasic_latent = torch.split(\n",
    "            latent_space, self.ls, dim=1\n",
    "        )\n",
    "        # Decode each signal independently\n",
    "        ecg_decoded = self.ecg_decoder(ecg_latent)\n",
    "        rsp_decoded = self.rsp_decoder(rsp_latent)\n",
    "        eda_tonic_decoded = self.eda_tonic_decoder(eda_tonic_latent)\n",
    "        eda_phasic_decoded = self.eda_phasic_decoder(eda_phasic_latent)\n",
    "\n",
    "        # Concatenate the decoded signals to form the output\n",
    "        reconstructed = torch.cat(\n",
    "            (ecg_decoded, rsp_decoded, eda_tonic_decoded, eda_phasic_decoded), dim=1\n",
    "        )\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_space = self.encode(x)\n",
    "        reconstructed = self.decode(latent_space)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoders to Each Participants' Baseline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = \"1s\"\n",
    "step_size = \"0.001s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through baseline data\n",
    "for file in os.listdir(\"./Physiological Preprocessed/Exp2\"):\n",
    "    participant = file.split(\"_\")[0]\n",
    "    if \"baseline\" not in file:\n",
    "        continue\n",
    "    elif os.path.exists(f\"./Convolutional Autoencoder Models/Takeover Time/{participant}_model.pth\"):\n",
    "        print(f\"Model for {file} already exists\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Loading {participant} data\")\n",
    "    print(f\"-\" * 50)\n",
    "\n",
    "    # load data\n",
    "    physiological_data = pd.read_csv(f\"./Physiological Preprocessed/Exp2/{file}\", usecols=[\"Time\", \"ECG_Clean\", \"RSP_Clean\", \"EDA_Tonic\", \"EDA_Phasic\"])\n",
    "    physiological_data[\"Time\"] = pd.to_timedelta(physiological_data[\"Time\"])\n",
    "    physiological_data.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    print(f\"Processing participant {participant} data\")\n",
    "\n",
    "    # Normalize the data\n",
    "    scalar = MinMaxScaler()\n",
    "    data = scalar.fit_transform(physiological_data)\n",
    "    physiological_data = pd.DataFrame(data, columns=physiological_data.columns, index=physiological_data.index)\n",
    "\n",
    "    # Split the data into sliding windows\n",
    "    X = []\n",
    "    len_segment = pd.Timedelta(segment_size) / pd.Timedelta(step_size)\n",
    "    while len(physiological_data) > 0:\n",
    "        start_index = physiological_data.index[0]\n",
    "        end_index = start_index + pd.Timedelta(segment_size)\n",
    "        segment = physiological_data[:end_index]\n",
    "        physiological_data = physiological_data[end_index:]\n",
    "\n",
    "        if len(segment) > len_segment:\n",
    "            length = len(segment) - len_segment\n",
    "            segment = segment.drop(segment.tail(int(length)).index)\n",
    "\n",
    "        if len(segment) == len_segment:\n",
    "            X.append(segment.to_numpy())\n",
    "\n",
    "    X = np.stack(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ConvAutoencoder().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # Training parameters\n",
    "    batch_size = 4\n",
    "    num_epochs = 300\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(X), batch_size):\n",
    "\n",
    "            # Get the batch and reshape it for Conv1d (batch, channels, sequence_length)\n",
    "            batch = X[i : i + batch_size].permute(0, 2, 1)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(X):.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(\"./Convolutional Autoencoder Models/Takeover Time\"):\n",
    "        os.makedirs(\"./Convolutional Autoencoder Models/Takeover Time\")\n",
    "    torch.save(model.state_dict(), f\"./Convolutional Autoencoder Models/Takeover Time/{participant}_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacles = [\"Deer\", \"Cone\", \"Frog\", \"Can\"]\n",
    "\n",
    "processed_physio_folder_path = \"./Physiological Preprocessed/\"\n",
    "\n",
    "exp2_folder_path = processed_physio_folder_path + \"Exp2\"\n",
    "\n",
    "exp2_takeover_times = pd.read_csv(\"./AdVitam/Exp2/Preprocessed/Physio and Driving/timestamps_obstacles.csv\")\n",
    "exp2_takeover_times.iloc[:, 2:] = exp2_takeover_times.iloc[:, 2:].apply(pd.to_timedelta, unit=\"s\")\n",
    "exp2_takeover_times[\"subject_id\"] = exp2_takeover_times[\"subject_id\"].apply(lambda x: x.split(\"T\")[0] + \"T\" + x.split(\"T\")[1].zfill(2))\n",
    "exp2_takeover_times[\"subject_id\"] = exp2_takeover_times[\"subject_id\"].astype(str)\n",
    "exp2_takeover_times.drop(columns=[\"label_st\"], inplace=True)\n",
    "exp2_takeover_times.sort_values(by=[\"subject_id\"], inplace=True)\n",
    "\n",
    "for column in exp2_takeover_times.columns:\n",
    "    if \"TrigObs\" in column:\n",
    "        exp2_takeover_times = exp2_takeover_times.rename(\n",
    "            columns={column: column.replace(\"TrigObs\", \"\") + \"TOR\"}\n",
    "        )\n",
    "    elif \"RepObs\" in column:\n",
    "        exp2_takeover_times = exp2_takeover_times.rename(\n",
    "            columns={column: column.replace(\"RepObs\", \"Response\")}\n",
    "        )\n",
    "\n",
    "for obstacle in obstacles:\n",
    "    exp2_takeover_times[\"TOT\" + obstacle] = (\n",
    "        exp2_takeover_times[\"Response\" + obstacle] - exp2_takeover_times[obstacle + \"TOR\"]\n",
    "    )\n",
    "\n",
    "exp2_takeover_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TOT is greater than 10 s, or less than 0 s, change to NaT\n",
    "for obstacle in obstacles:\n",
    "    exp2_takeover_times.loc[\n",
    "        (exp2_takeover_times[\"TOT\" + obstacle] > pd.Timedelta(\"10s\"))\n",
    "        | (exp2_takeover_times[\"TOT\" + obstacle] < pd.Timedelta(\"0s\")),\n",
    "        \"TOT\" + obstacle,\n",
    "    ] = pd.NaT\n",
    "\n",
    "\n",
    "# print max and min takeover times\n",
    "for obstacle in obstacles:\n",
    "    print(obstacle)\n",
    "    print(\"Max takeover time: \", exp2_takeover_times[\"TOT\" + obstacle].max())\n",
    "    print(\"Min takeover time: \", exp2_takeover_times[\"TOT\" + obstacle].min())\n",
    "    print(\"Mean takeover time: \", exp2_takeover_times[\"TOT\" + obstacle].mean())\n",
    "    print(\"Median takeover time: \", exp2_takeover_times[\"TOT\" + obstacle].median())\n",
    "    print(\"Standard deviation of takeover time: \", exp2_takeover_times[\"TOT\" + obstacle].std())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_observations(phsyiological_data_folder, include_latent_space, jump, takeover_times, tot):\n",
    "    \"\"\"\n",
    "    Create the observations for the slow and fast takeover times.\n",
    "\n",
    "    Args:\n",
    "        phsyiological_data_dictionary (dict): A dictionary containing the segmented physiological data files.\n",
    "        takeover_times (pd.DataFrame): A DataFrame containing the takeover times.\n",
    "        driver_demographic_data (pd.DataFrame): A DataFrame containing the driver demographic data.\n",
    "        window_length (int): The length of the window in minutes.\n",
    "        window_step (int): The step size for the window\n",
    "\n",
    "    Returns:\n",
    "        list: A list of observations for the slow takeover times.\n",
    "        list: A list of observations for the fast takeover times.\n",
    "    \"\"\"\n",
    "\n",
    "    slow_observations = []\n",
    "    fast_observations = []\n",
    "\n",
    "    for file in os.listdir(phsyiological_data_folder):\n",
    "        # Split the file name into the participant and period\n",
    "        f = file.split(\"_\")\n",
    "        participant = f[0]\n",
    "        period = f[1].split(\".\")[0]\n",
    "\n",
    "        if period != \"driving\":\n",
    "            continue\n",
    "\n",
    "        # print(f\"Processing {participant} data\")\n",
    "\n",
    "        # Physiological data\n",
    "        physio = pd.read_csv(phsyiological_data_folder + \"/\" + file, usecols=[\"Time\", \"ECG_Clean\", \"RSP_Clean\", \"EDA_Tonic\", \"EDA_Phasic\"])\n",
    "        physio[\"Time\"] = pd.to_timedelta(physio[\"Time\"])\n",
    "        physio.set_index(\"Time\", inplace=True)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = ConvAutoencoder().to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        model.load_state_dict(torch.load(f\"./Convolutional Autoencoder Models/Takeover Time/{participant}_model.pth\"))\n",
    "        model.eval()\n",
    "\n",
    "        scalar = MinMaxScaler()\n",
    "        data = scalar.fit_transform(physio)\n",
    "        physio = pd.DataFrame(data, columns=physio.columns, index=physio.index)\n",
    "\n",
    "        # Takeover Times\n",
    "        participant_takeover_times = takeover_times[\n",
    "            takeover_times[\"subject_id\"] == participant\n",
    "        ].copy()\n",
    "\n",
    "        # convert every value to timedelta\n",
    "        participant_takeover_times.iloc[:, 1:] = participant_takeover_times.iloc[:, 1:].apply(\n",
    "            pd.to_timedelta, args=(\"s\",), errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        for obstacle in obstacles:\n",
    "            takeover_time = participant_takeover_times[f\"TOT{obstacle}\"].values[0]\n",
    "\n",
    "            if pd.isna(takeover_time) or pd.isnull(takeover_time):\n",
    "                continue\n",
    "\n",
    "            obstacle_trigger_time = pd.to_timedelta(participant_takeover_times[f\"{obstacle}TOR\"].values[0], unit=\"s\")\n",
    "\n",
    "            if pd.isnull(obstacle_trigger_time):\n",
    "                continue\n",
    "\n",
    "            # Get the physiological data for the period\n",
    "            obstacle_physio = physio.loc[obstacle_trigger_time - pd.Timedelta(seconds=4):obstacle_trigger_time + pd.Timedelta(seconds=8)]\n",
    "\n",
    "            step_size = \"0.001s\"\n",
    "            observations = []\n",
    "            len_segment = pd.Timedelta(segment_size) / pd.Timedelta(step_size)\n",
    "            while len(obstacle_physio) > 0:\n",
    "                start_index = obstacle_physio.index[0]\n",
    "                end_index = start_index + pd.Timedelta(segment_size)\n",
    "                segmented_signal = obstacle_physio[:end_index]\n",
    "                obstacle_physio = obstacle_physio[start_index + pd.Timedelta(jump) :]\n",
    "\n",
    "                if len(segmented_signal) > len_segment:\n",
    "                    length = len(segmented_signal) - len_segment\n",
    "                    segmented_signal = segmented_signal.drop(segmented_signal.tail(int(length)).index)\n",
    "\n",
    "                if len(segmented_signal) == len_segment:\n",
    "                    segmented_signal = torch.tensor(segmented_signal.to_numpy(), dtype=torch.float32).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        latent_space = model.encode(segmented_signal.permute(1, 0).unsqueeze(0))\n",
    "                        reconstructed_signal = model.decode(latent_space).squeeze(0).permute(1, 0)\n",
    "                    loss = criterion(reconstructed_signal, segmented_signal)\n",
    "                    if include_latent_space:\n",
    "                        pca_signal = PCA(n_components=1).fit_transform(latent_space.cpu().detach().numpy().squeeze()).squeeze()\n",
    "                        segmented_observations = np.concatenate((pca_signal, [loss.item()]))\n",
    "                    else:\n",
    "                        segmented_observations.append(loss.item())\n",
    "\n",
    "                    observations.append(segmented_observations)\n",
    "\n",
    "            observations = np.stack(observations)\n",
    "\n",
    "            if len(observations) == 0:\n",
    "                continue\n",
    "\n",
    "            # print(numpy_data)\n",
    "            if takeover_time > pd.to_timedelta(\"0 days 00:00:0\" + tot):\n",
    "                slow_observations.append(observations)\n",
    "\n",
    "            else:\n",
    "                fast_observations.append(observations)\n",
    "\n",
    "    return slow_observations, fast_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(\n",
    "    slow_observations,\n",
    "    fast_observations,\n",
    "    n_components_slow,\n",
    "    n_components_fast,\n",
    "    n_mix_slow,\n",
    "    n_mix_fast,\n",
    "    covariance_type,\n",
    "    include_latent_space\n",
    "):\n",
    "    iterations = 100\n",
    "\n",
    "    accuracies = []\n",
    "    true_positives_list = []\n",
    "    false_positives_list = []\n",
    "    true_negatives_list = []\n",
    "    false_negatives_list = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration: {i}\")\n",
    "\n",
    "        # split the data\n",
    "        slow_observations_train, slow_observations_test = train_test_split(\n",
    "            slow_observations, test_size=0.3\n",
    "        )\n",
    "        fast_observations_train, fast_observations_test = train_test_split(\n",
    "            fast_observations, test_size=0.3\n",
    "        )\n",
    "\n",
    "        # concatenate the observations\n",
    "        X_slow = None\n",
    "        X_slow_lengths = []\n",
    "        for data in slow_observations_train:\n",
    "            if X_slow is None:\n",
    "                X_slow = data\n",
    "            else:\n",
    "                X_slow = np.concatenate((X_slow, data))\n",
    "            X_slow_lengths.append(len(data))\n",
    "\n",
    "        X_fast = None\n",
    "        X_fast_lengths = []\n",
    "        for data in fast_observations_train:\n",
    "            if X_fast is None:\n",
    "                X_fast = data\n",
    "            else:\n",
    "                X_fast = np.concatenate((X_fast, data))\n",
    "            X_fast_lengths.append(len(data))\n",
    "\n",
    "        # initialize and fit the models\n",
    "        slow_model = hmm.GMMHMM(\n",
    "            n_components=n_components_slow, n_mix=n_mix_slow, covariance_type=covariance_type\n",
    "        )\n",
    "        fast_model = hmm.GMMHMM(\n",
    "            n_components=n_components_fast, n_mix=n_mix_fast, covariance_type=covariance_type\n",
    "        )\n",
    "\n",
    "        # fit the models\n",
    "        if not include_latent_space:\n",
    "            X_slow = X_slow.reshape(-1, 1)\n",
    "            X_fast = X_fast.reshape(-1, 1)\n",
    "        slow_model.fit(X_slow, X_slow_lengths)\n",
    "        fast_model.fit(X_fast, X_fast_lengths)\n",
    "\n",
    "        # score the models\n",
    "        accuracy = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "\n",
    "        for _, observation in enumerate(slow_observations_test):\n",
    "            if not include_latent_space:\n",
    "                observation = observation.reshape(-1, 1)\n",
    "\n",
    "            if slow_model.score(observation) > fast_model.score(observation):\n",
    "                accuracy += 1\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "        for _, observation in enumerate(fast_observations_test):\n",
    "            if not include_latent_space:\n",
    "                observation = observation.reshape(-1, 1)\n",
    "\n",
    "            if fast_model.score(observation) > slow_model.score(observation):\n",
    "                accuracy += 1\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        accuracy = accuracy / (len(slow_observations_test) + len(fast_observations_test))\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        true_positives_list.append(tp)\n",
    "        false_positives_list.append(fp)\n",
    "        true_negatives_list.append(tn)\n",
    "        false_negatives_list.append(fn)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            print(f\"True Positives: {tp}\")\n",
    "            print(f\"False Positives: {fp}\")\n",
    "            print(f\"True Negatives: {tn}\")\n",
    "            print(f\"False Negatives: {fn}\")\n",
    "\n",
    "    return (\n",
    "        accuracies,\n",
    "        true_positives_list,\n",
    "        false_positives_list,\n",
    "        true_negatives_list,\n",
    "        false_negatives_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = \"3\"\n",
    "include_latent_space = [True, False]\n",
    "jump_size = [\"1s\", \"500ms\", \"100ms\"]\n",
    "n_components_slow = [1, 2, 3, 4]\n",
    "n_mix_slow = [1, 2, 3, 4]\n",
    "n_components_fast = [1, 2, 3, 4]\n",
    "n_mix_fast = [1, 2, 3, 4]\n",
    "covariance_types = [\"full\", \"diag\", \"spherical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for include_latent in include_latent_space:\n",
    "    for jump in jump_size:\n",
    "        print(f\"Collecting observations every {jump} {'with latent space' if include_latent else ''}\")\n",
    "        slow_observations, fast_observations = collect_observations(exp2_folder_path, include_latent, jump, exp2_takeover_times, tot)\n",
    "        print(f\"Collected {len(slow_observations)} slow takeover times and {len(fast_observations)} fast takeover times\")\n",
    "\n",
    "        for n_slow in n_components_slow:\n",
    "            for m_slow in n_mix_slow:\n",
    "                for n_fast in n_components_fast:\n",
    "                    for m_fast in n_mix_fast:\n",
    "                        for cov in covariance_types:\n",
    "\n",
    "                            # Check if the results already exist\n",
    "                            if os.path.exists(\"results-tot-ehmm.csv\"):\n",
    "                                results_df = pd.read_csv(\"results-tot-ehmm.csv\")\n",
    "                                results = results_df[\n",
    "                                        (results_df[\"TOT\"] == int(tot)) \n",
    "                                        & (results_df[\"Latent Space\"] == include_latent)\n",
    "                                        & (results_df[\"Jump\"] == jump)\n",
    "                                        & (results_df[\"Components Slow\"] == n_slow)\n",
    "                                        & (results_df[\"Mixtures Slow\"] == m_slow)\n",
    "                                        & (results_df[\"Components Fast\"] == n_fast)\n",
    "                                        & (results_df[\"Mixtures Fast\"] == m_fast)\n",
    "                                        & (results_df[\"Covariance\"] == cov)\n",
    "                                    ]\n",
    "                                if not results.empty:\n",
    "                                    print(\"Results already exist for these parameters.\")\n",
    "                                    continue\n",
    "\n",
    "                            print(\"-------------------------------------------------\")\n",
    "                            print(f\"Slow Components: {n_slow}, Slow Mixtures: {m_slow}, Fast Components: {n_fast}, Fast Mixtures: {m_fast}, Covariance: {cov}\")\n",
    "                            try:\n",
    "                                accuracies, true_positives_list, false_positives_list, true_negatives_list, false_negatives_list = accuracy(slow_observations, fast_observations, n_slow, n_fast, m_slow, m_fast, cov, include_latent )\n",
    "\n",
    "                                # Accuracy\n",
    "                                print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
    "                                print(f\"Standard Deviation: {np.std(accuracies)}\")\n",
    "                                print(f\"Max Accuracy: {np.max(accuracies)}\")\n",
    "                                print(f\"Min Accuracy: {np.min(accuracies)}\")\n",
    "\n",
    "                                # Find the index of the max accuracy\n",
    "                                max_accuracy_index = accuracies.index(np.max(accuracies))\n",
    "                                tp = true_positives_list[max_accuracy_index]\n",
    "                                print(f\"True Positives: {tp}\")\n",
    "                                fp = false_positives_list[max_accuracy_index]\n",
    "                                print(f\"False Positives: {fp}\")\n",
    "                                tn = true_negatives_list[max_accuracy_index]\n",
    "                                print(f\"True Negatives: {tn}\")\n",
    "                                fn = false_negatives_list[max_accuracy_index]\n",
    "                                print(f\"False Negatives: {fn}\")\n",
    "\n",
    "                                # check  if any of the values are zero\n",
    "                                if tp + fp == 0 or tp + fn == 0:\n",
    "                                    precision = 0\n",
    "                                    recall = 0\n",
    "                                    f1_score = 0\n",
    "                                else:\n",
    "                                    # Precision, Recall, and F1 Score\n",
    "                                    precision = tp / (tp + fp)\n",
    "                                    recall = tp / (tp + fn)\n",
    "                                    f1_score = 2 * precision * recall / (precision + recall)\n",
    "                                print(f'Precision: {precision}')\n",
    "                                print(f'Recall: {recall}') \n",
    "                                print(f'F1 Score: {f1_score}')\n",
    "                                print(\"-------------------------------------------------\")\n",
    "                                print()\n",
    "                                results = [tot, include_latent, jump, n_slow, m_slow, n_fast, m_fast, cov, np.mean(accuracies), np.std(accuracies), np.max(accuracies), np.min(accuracies), tp, fp, tn, fn, precision, recall, f1_score]\n",
    "                            except:\n",
    "                                results = [tot, include_latent, jump, n_slow, m_slow, n_fast, m_fast, cov, None, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "                            results_columns = [\"TOT\", \"Latent Space\", \"Jump\", \"Components Slow\", \"Mixtures Slow\", \"Components Fast\", \"Mixtures Fast\", \"Covariance\", \"Mean Accuracy\", \"Standard Deviation\", \"Max Accuracy\", \"Min Accuracy\", \"True Positives\", \"False Positives\", \"True Negatives\", \"False Negatives\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "\n",
    "                            if os.path.exists(\"results-tot-ehmm.csv\"):\n",
    "                                results_df = pd.read_csv(\"results-tot-ehmm.csv\")\n",
    "                                results_df = pd.concat([results_df, pd.DataFrame([results], columns=results_columns)])\n",
    "                                results_df.to_csv(\"results-tot-ehmm.csv\", index=False)\n",
    "                            else:\n",
    "                                pd.DataFrame([results], columns=results_columns).to_csv(\"results-tot-ehmm.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
