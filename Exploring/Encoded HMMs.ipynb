{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Convolutional Autoencoder (CAE) architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "        self.ls = 256\n",
    "\n",
    "        # Define separate encoders for each signal\n",
    "        self.ecg_encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.rsp_encoder = self.ecg_encoder\n",
    "        self.eda_tonic_encoder = self.ecg_encoder\n",
    "        self.eda_phasic_encoder = self.ecg_encoder\n",
    "\n",
    "        # Define separate decoders for each signal\n",
    "        self.ecg_decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.rsp_decoder = self.ecg_decoder\n",
    "        self.eda_tonic_decoder = self.ecg_decoder\n",
    "        self.eda_phasic_decoder = self.ecg_decoder\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Split input by channel for independent processing\n",
    "        ecg = x[:, 0, :].unsqueeze(1)\n",
    "        rsp = x[:, 1, :].unsqueeze(1)\n",
    "        eda_tonic = x[:, 2, :].unsqueeze(1)\n",
    "        eda_phasic = x[:, 3, :].unsqueeze(1)\n",
    "\n",
    "        # Encode each signal independently\n",
    "        ecg_encoded = self.ecg_encoder(ecg)\n",
    "        rsp_encoded = self.rsp_encoder(rsp)\n",
    "        eda_tonic_encoded = self.eda_tonic_encoder(eda_tonic)\n",
    "        eda_phasic_encoded = self.eda_phasic_encoder(eda_phasic)\n",
    "\n",
    "        # Concatenate the latent representations along the last dimension\n",
    "        latent_space = torch.cat(\n",
    "            (ecg_encoded, rsp_encoded, eda_tonic_encoded, eda_phasic_encoded), dim=1\n",
    "        )\n",
    "\n",
    "        return latent_space\n",
    "\n",
    "    def decode(self, latent_space):\n",
    "        # Split latent space back into separate channels\n",
    "        ecg_latent, rsp_latent, eda_tonic_latent, eda_phasic_latent = torch.split(\n",
    "            latent_space, self.ls, dim=1\n",
    "        )\n",
    "        # Decode each signal independently\n",
    "        ecg_decoded = self.ecg_decoder(ecg_latent)\n",
    "        rsp_decoded = self.rsp_decoder(rsp_latent)\n",
    "        eda_tonic_decoded = self.eda_tonic_decoder(eda_tonic_latent)\n",
    "        eda_phasic_decoded = self.eda_phasic_decoder(eda_phasic_latent)\n",
    "\n",
    "        # Concatenate the decoded signals to form the output\n",
    "        reconstructed = torch.cat(\n",
    "            (ecg_decoded, rsp_decoded, eda_tonic_decoded, eda_phasic_decoded), dim=1\n",
    "        )\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_space = self.encode(x)\n",
    "        reconstructed = self.decode(latent_space)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoders to Each Participants' Baseline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = \"12s\"\n",
    "step_size = \"0.001s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through baseline data\n",
    "for file in os.listdir(\"../Physiological Preprocessed/Exp2\"):\n",
    "    participant = file.split(\"_\")[0]\n",
    "    if \"baseline\" not in file:\n",
    "        continue\n",
    "    elif os.path.exists(f\"../Convolutional Autoencoder Models/{participant}_model.pth\"):\n",
    "        print(f\"Model for {file} already exists\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Loading {participant} data\")\n",
    "    print(f\"-\" * 50)\n",
    "\n",
    "    # load data\n",
    "    physiological_data = pd.read_csv(f\"../Physiological Preprocessed/Exp2/{file}\", usecols=[\"Time\", \"ECG_Clean\", \"RSP_Clean\", \"EDA_Tonic\", \"EDA_Phasic\"])\n",
    "    physiological_data[\"Time\"] = pd.to_timedelta(physiological_data[\"Time\"])\n",
    "    physiological_data.set_index(\"Time\", inplace=True)\n",
    "\n",
    "    print(f\"Processing participant {participant} data\")\n",
    "\n",
    "    # Normalize the data\n",
    "    scalar = MinMaxScaler()\n",
    "    data = scalar.fit_transform(physiological_data)\n",
    "    physiological_data = pd.DataFrame(data, columns=physiological_data.columns, index=physiological_data.index)\n",
    "\n",
    "    # Split the data into sliding windows\n",
    "    X = []\n",
    "    len_segment = pd.Timedelta(segment_size) / pd.Timedelta(step_size)\n",
    "    while len(physiological_data) > 0:\n",
    "        start_index = physiological_data.index[0]\n",
    "        end_index = start_index + pd.Timedelta(segment_size)\n",
    "        segment = physiological_data[:end_index]\n",
    "        physiological_data = physiological_data[end_index:]\n",
    "\n",
    "        if len(segment) > len_segment:\n",
    "            length = len(segment) - len_segment\n",
    "            segment = segment.drop(segment.tail(int(length)).index)\n",
    "\n",
    "        if len(segment) == len_segment:\n",
    "            X.append(segment.to_numpy())\n",
    "\n",
    "    X = np.stack(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "\n",
    "    features = X.shape[2]\n",
    "\n",
    "    # Initialize the model\n",
    "    model = ConvAutoencoder().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "    # Training parameters\n",
    "    batch_size = 4\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(X), batch_size):\n",
    "\n",
    "            # Get the batch and reshape it for Conv1d (batch, channels, sequence_length)\n",
    "            batch = X[i : i + batch_size].permute(0, 2, 1)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(X):.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(\"../Convolutional Autoencoder Models\"):\n",
    "        os.makedirs(\"../Convolutional Autoencoder Models\")\n",
    "    torch.save(model.state_dict(), f\"../Convolutional Autoencoder Models/{participant}_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the physiological timestamps / takeover times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_physio_folder_path = \"../Physiological Preprocessed/\"\n",
    "\n",
    "exp2_folder_path = processed_physio_folder_path + \"Exp2\"\n",
    "\n",
    "exp2_takeover_times = pd.read_csv(\n",
    "    \"../AdVitam/Exp2/Preprocessed/Physio and Driving/timestamps_obstacles.csv\"\n",
    ")\n",
    "exp2_takeover_times.iloc[:, 2:] = exp2_takeover_times.iloc[:, 2:].apply(pd.to_timedelta, unit=\"s\")\n",
    "exp2_takeover_times[\"subject_id\"] = exp2_takeover_times[\"subject_id\"].apply(\n",
    "    lambda x: x.split(\"T\")[0] + \"T\" + x.split(\"T\")[1].zfill(2)\n",
    ")\n",
    "exp2_takeover_times[\"subject_id\"] = exp2_takeover_times[\"subject_id\"].astype(str)\n",
    "exp2_takeover_times.drop(columns=[\"label_st\"], inplace=True)\n",
    "exp2_takeover_times.sort_values(by=[\"subject_id\"], inplace=True)\n",
    "\n",
    "for column in exp2_takeover_times.columns:\n",
    "    if \"TrigObs\" in column:\n",
    "        exp2_takeover_times = exp2_takeover_times.rename(\n",
    "            columns={column: column.replace(\"TrigObs\", \"\") + \"TOR\"}\n",
    "        )\n",
    "    elif \"RepObs\" in column:\n",
    "        exp2_takeover_times = exp2_takeover_times.rename(\n",
    "            columns={column: column.replace(\"RepObs\", \"Response\")}\n",
    "        )\n",
    "\n",
    "exp2_takeover_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the observations for the slow and fast takeover times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_observations(\n",
    "    exp2_folder_path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the observations for the slow and fast takeover times.\n",
    "\n",
    "    Args:\n",
    "        phsyiological_data_dictionary (dict): A dictionary containing the segmented physiological data files.\n",
    "        takeover_times (pd.DataFrame): A DataFrame containing the takeover times.\n",
    "        driver_demographic_data (pd.DataFrame): A DataFrame containing the driver demographic data.\n",
    "        window_length (int): The length of the window in minutes.\n",
    "        window_step (int): The step size for the window\n",
    "        step_sizes (list): A list of step sizes for the window.\n",
    "        tot (str): The threshold for the takeover time.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of observations for the slow takeover times.\n",
    "        list: A list of observations for the fast takeover times.\n",
    "    \"\"\"\n",
    "\n",
    "    driving_observations_data = []\n",
    "    takeover_observations_data = []\n",
    "\n",
    "    # Exp2\n",
    "    for file in os.listdir(exp2_folder_path):\n",
    "        # Split the file name into the participant and period\n",
    "        f = file.split(\"_\")\n",
    "        participant = f[0]\n",
    "        period = f[1].split(\".\")[0]\n",
    "\n",
    "        if \"baseline\" in period:\n",
    "            continue\n",
    "        elif \"driving\" in period:\n",
    "            print(participant)\n",
    "            print(f\"-\" * 50)\n",
    "\n",
    "            # Process the physiological data\n",
    "            experiment_physio = pd.read_csv(exp2_folder_path + \"/\" + file, usecols=[\"Time\", \"ECG_Clean\", \"RSP_Clean\", \"EDA_Tonic\", \"EDA_Phasic\"])\n",
    "            experiment_physio[\"Time\"] = pd.to_timedelta(experiment_physio[\"Time\"])\n",
    "            experiment_physio.set_index(\"Time\", inplace=True)\n",
    "\n",
    "            # Normalize the data\n",
    "            scalar = MinMaxScaler()\n",
    "            data = scalar.fit_transform(experiment_physio)\n",
    "            experiment_physio = pd.DataFrame(data, columns=experiment_physio.columns, index=experiment_physio.index)\n",
    "\n",
    "            # Obstacle Trigger Times\n",
    "            participant_takeover_times = exp2_takeover_times[exp2_takeover_times[\"subject_id\"] == participant].copy()\n",
    "            participant_takeover_times.iloc[:, 1:] = participant_takeover_times.iloc[:, 1:].apply(pd.to_timedelta, args=(\"s\",), errors=\"coerce\")\n",
    "\n",
    "            obstacles = [\"Deer\", \"Cone\", \"Frog\", \"Can\"]\n",
    "            for obstacle in obstacles:\n",
    "                print(obstacle)\n",
    "\n",
    "                # Obstacle Trigger Time\n",
    "                obstacle_trigger_time = pd.to_timedelta(participant_takeover_times[f\"{obstacle}TOR\"].values[0], unit=\"s\")\n",
    "                minute_before_obstacle = obstacle_trigger_time - pd.Timedelta(seconds=60)\n",
    "\n",
    "                # If the obstacle trigger time is null, skip the obstacle\n",
    "                if pd.isnull(obstacle_trigger_time):\n",
    "                    continue\n",
    "                if pd.isnull(minute_before_obstacle):\n",
    "                    continue\n",
    "\n",
    "                # Observations 1 minute before and after the obstacle\n",
    "                driving_observations_before_obstacle = experiment_physio.loc[\n",
    "                    minute_before_obstacle - pd.Timedelta(seconds=4) : minute_before_obstacle\n",
    "                ].copy()\n",
    "                driving_observations_after_obstacle = experiment_physio.loc[\n",
    "                    minute_before_obstacle : minute_before_obstacle + pd.Timedelta(seconds=8)\n",
    "                ].copy()\n",
    "\n",
    "                # Observations 3 seconds before and after the obstacle\n",
    "                takeover_observations_before_obstacle = experiment_physio.loc[\n",
    "                    obstacle_trigger_time - pd.Timedelta(seconds=4) : obstacle_trigger_time\n",
    "                ].copy()\n",
    "                takeover_observations_after_obstacle = experiment_physio.loc[\n",
    "                    obstacle_trigger_time : obstacle_trigger_time + pd.Timedelta(seconds=8)\n",
    "                ].copy()\n",
    "\n",
    "                # Check if the last observation of before obstacle is the same as the first observation of after obstacle\n",
    "                if (\n",
    "                    len(driving_observations_before_obstacle) > 0\n",
    "                    and len(driving_observations_after_obstacle) > 0\n",
    "                    and driving_observations_before_obstacle.tail(1).index\n",
    "                    == driving_observations_after_obstacle.head(1).index\n",
    "                ):\n",
    "                    # drop the first observation of after obstacle\n",
    "                    driving_observations_after_obstacle = driving_observations_after_obstacle.iloc[1:]\n",
    "\n",
    "                if (\n",
    "                    len(takeover_observations_before_obstacle) > 0\n",
    "                    and len(takeover_observations_after_obstacle) > 0\n",
    "                    and takeover_observations_before_obstacle.tail(1).index\n",
    "                    == takeover_observations_after_obstacle.head(1).index\n",
    "                ):\n",
    "                    # drop the first observation of after obstacle\n",
    "                    takeover_observations_after_obstacle = (takeover_observations_after_obstacle.iloc[1:])\n",
    "\n",
    "                # Check if the length of the observations is 3000\n",
    "                if len(driving_observations_before_obstacle) > 4000:\n",
    "                    # drop the first n rows\n",
    "                    n = len(driving_observations_before_obstacle) - 4000\n",
    "                    driving_observations_before_obstacle = (\n",
    "                        driving_observations_before_obstacle.iloc[n:]\n",
    "                    )\n",
    "                elif len(driving_observations_before_obstacle) < 4000:\n",
    "                    continue\n",
    "\n",
    "                if len(driving_observations_after_obstacle) > 8000:\n",
    "                    # drop the last n rows\n",
    "                    driving_observations_after_obstacle = driving_observations_after_obstacle.iloc[\n",
    "                        :8000\n",
    "                    ]\n",
    "                elif len(driving_observations_after_obstacle) < 8000:\n",
    "                    continue\n",
    "\n",
    "                if len(takeover_observations_before_obstacle) > 4000:\n",
    "                    # drop the first n rows\n",
    "                    n = len(takeover_observations_before_obstacle) - 4000\n",
    "                    takeover_observations_before_obstacle = (\n",
    "                        takeover_observations_before_obstacle.iloc[n:]\n",
    "                    )\n",
    "                elif len(takeover_observations_before_obstacle) < 4000:\n",
    "                    continue\n",
    "\n",
    "                if len(takeover_observations_after_obstacle) > 8000:\n",
    "                    # drop the last n rows\n",
    "                    takeover_observations_after_obstacle = (\n",
    "                        takeover_observations_after_obstacle.iloc[:8000]\n",
    "                    )\n",
    "                elif len(takeover_observations_after_obstacle) < 8000:\n",
    "                    continue\n",
    "\n",
    "                # Load the participant's model\n",
    "                model = ConvAutoencoder().to(device)\n",
    "                model.load_state_dict(torch.load(f\"../Convolutional Autoencoder Models/{participant}_model.pth\"))\n",
    "                model.eval()\n",
    "\n",
    "                # Combine the observations\n",
    "                driving_observations = pd.concat(\n",
    "                    [driving_observations_before_obstacle, driving_observations_after_obstacle]\n",
    "                )\n",
    "                takeover_observations = pd.concat(\n",
    "                    [takeover_observations_before_obstacle, takeover_observations_after_obstacle]\n",
    "                )\n",
    "\n",
    "                # Encode the observations\n",
    "                driving_observations = driving_observations.to_numpy()\n",
    "                input_observations = torch.tensor(driving_observations, dtype=torch.float32).to(device)\n",
    "                latent_space = model.encode(input_observations.permute(1, 0).unsqueeze(0))\n",
    "                driving_observations = latent_space.squeeze(0).permute(1, 0)\n",
    "\n",
    "                # add the reconstruction error to the observations\n",
    "                output = model.decode(latent_space).squeeze(0).permute(1, 0)\n",
    "                driving_observations_error = torch.nn.functional.mse_loss(output, input_observations,) * 1000 # scale the error\n",
    "                print(f\"Driving Observations Error: {driving_observations_error}\")\n",
    "                error_expanded = driving_observations_error.expand(driving_observations.shape[0], 1)\n",
    "                driving_observations = torch.cat((driving_observations, error_expanded), 1)\n",
    "\n",
    "                takeover_observations = takeover_observations.to_numpy()\n",
    "                input_observations = torch.tensor(takeover_observations, dtype=torch.float32).to(device)\n",
    "                latent_space = model.encode(input_observations.permute(1, 0).unsqueeze(0))\n",
    "                takeover_observations = latent_space.squeeze(0).permute(1, 0)\n",
    "\n",
    "                # add the reconstruction error to the observations\n",
    "                output = model.decode(latent_space).squeeze(0).permute(1, 0)\n",
    "                takeover_observations_error = torch.nn.functional.mse_loss(output, input_observations,) * 1000 # scale the error\n",
    "                print(f\"Takeover Observations Error: {takeover_observations_error}\")\n",
    "                error_expanded = takeover_observations_error.expand(takeover_observations.shape[0], 1)\n",
    "                takeover_observations = torch.cat((takeover_observations, error_expanded), 1)\n",
    "\n",
    "                driving_observations_data.append(driving_observations)\n",
    "                takeover_observations_data.append(takeover_observations)\n",
    "\n",
    "        print(f\"-\" * 50)\n",
    "\n",
    "    return driving_observations_data, takeover_observations_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Collecting Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the observations for these parameters\n",
    "print(f\"Collecting Features\")\n",
    "driving_observations, takeover_observations = collect_observations(exp2_folder_path)\n",
    "print(f\"Collected {len(driving_observations)} driving observations and {len(takeover_observations)} takeover observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_observations_train, driving_observations_test = train_test_split(\n",
    "    driving_observations, test_size=0.3\n",
    ")\n",
    "\n",
    "takeover_observations_train, takeover_observations_test = train_test_split(\n",
    "    takeover_observations, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the observations\n",
    "X_driving = None\n",
    "X_driving_lengths = []\n",
    "for data in driving_observations_train:\n",
    "    if X_driving is None:\n",
    "        X_driving = data.detach().numpy()\n",
    "    else:\n",
    "        X_driving = np.concatenate((X_driving, data.detach().numpy()), axis=1)\n",
    "    X_driving_lengths.append(data.shape[1])\n",
    "X_driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_takeover = None\n",
    "X_takeover_lengths = []\n",
    "for data in takeover_observations_train:\n",
    "    if X_takeover is None:\n",
    "        X_takeover = data.detach().numpy()\n",
    "    else:\n",
    "        X_takeover = np.concatenate((X_takeover, data.detach().numpy()), axis=1)\n",
    "    X_takeover_lengths.append(data.shape[1])\n",
    "X_takeover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_components_slow = [1, 2, 3, 4]\n",
    "n_mix_slow = [1, 2, 3, 4]\n",
    "n_components_fast = [1, 2, 3, 4]\n",
    "n_mix_fast = [1, 2, 3, 4]\n",
    "covariance_types = [\"full\", \"diag\", \"spherical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize and fit the models\n",
    "driving_model = hmm.GMMHMM(\n",
    "    n_components=1, n_mix=1, covariance_type=\"full\"\n",
    ")\n",
    "takeover_model = hmm.GMMHMM(\n",
    "    n_components=1, n_mix=1, covariance_type=\"full\",\n",
    ")\n",
    "\n",
    "# fit the models\n",
    "driving_model.fit(X_driving, X_driving_lengths)\n",
    "takeover_model.fit(X_takeover, X_takeover_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the models\n",
    "accuracy = 0\n",
    "tp = 0\n",
    "fp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "\n",
    "for _, observation in enumerate(driving_observations_test):\n",
    "    observation = observation.detach().numpy()\n",
    "\n",
    "    if driving_model.score(observation) > takeover_model.score(observation):\n",
    "        accuracy += 1\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "for _, observation in enumerate(takeover_observations_test):\n",
    "    observation = observation.detach().numpy()\n",
    "\n",
    "    if takeover_model.score(observation) > driving_model.score(observation):\n",
    "        accuracy += 1\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "accuracy = accuracy / (len(driving_observations_test) + len(takeover_observations_test))\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate over the model parameters\n",
    "for n_slow in n_components_slow:\n",
    "    for m_slow in n_mix_slow:\n",
    "        for n_fast in n_components_fast:\n",
    "            for m_fast in n_mix_fast:\n",
    "                for cov in covariance_types:\n",
    "\n",
    "                    # check if the results for these parameters already exist\n",
    "                    if os.path.exists(\"results-cehmm.csv\"):\n",
    "                        results_df = pd.read_csv(\"results-thmm.csv\")\n",
    "                        results = results_df[\n",
    "                            (results_df[\"Components Slow\"] == n_slow)\n",
    "                            & (results_df[\"Mixtures Slow\"] == m_slow)\n",
    "                            & (results_df[\"Components Fast\"] == n_fast)\n",
    "                            & (results_df[\"Mixtures Fast\"] == m_fast)\n",
    "                            & (results_df[\"Covariance Type\"] == cov)\n",
    "                        ]\n",
    "                        if not results.empty:\n",
    "                            print(\"Results already exist for these parameters.\")\n",
    "                            continue\n",
    "\n",
    "                    print(\"-------------------------------------------------\")\n",
    "                    print(f\"Slow Components: {n_slow}, Slow Mixtures: {m_slow}, Fast Components: {n_fast}, Fast Mixtures: {m_fast}, Covariance Type: {cov}\")\n",
    "                    try:\n",
    "                        accuracies, true_positives_list, false_positives_list, true_negatives_list, false_negatives_list = accuracy(driving_observations, takeover_observations, n_slow, n_fast, m_slow, m_fast, cov)\n",
    "\n",
    "                        # Accuracy\n",
    "                        print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
    "                        print(f\"Standard Deviation: {np.std(accuracies)}\")\n",
    "                        print(f\"Max Accuracy: {np.max(accuracies)}\")\n",
    "                        print(f\"Min Accuracy: {np.min(accuracies)}\")\n",
    "\n",
    "                        # Find the index of the max accuracy\n",
    "                        max_accuracy_index = accuracies.index(np.max(accuracies))\n",
    "                        tp = true_positives_list[max_accuracy_index]\n",
    "                        print(f\"True Positives: {tp}\")\n",
    "                        fp = false_positives_list[max_accuracy_index]\n",
    "                        print(f\"False Positives: {fp}\")\n",
    "                        tn = true_negatives_list[max_accuracy_index]\n",
    "                        print(f\"True Negatives: {tn}\")\n",
    "                        fn = false_negatives_list[max_accuracy_index]\n",
    "                        print(f\"False Negatives: {fn}\")\n",
    "\n",
    "                        # check  if any of the values are zero\n",
    "                        if tp + fp == 0 or tp + fn == 0:\n",
    "                            precision = 0\n",
    "                            recall = 0\n",
    "                            f1_score = 0\n",
    "                        else:\n",
    "                            # Precision, Recall, and F1 Score\n",
    "                            precision = tp / (tp + fp)\n",
    "                            recall = tp / (tp + fn)\n",
    "                            f1_score = 2 * precision * recall / (precision + recall)\n",
    "                        print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}')\n",
    "                        print(\"-------------------------------------------------\")\n",
    "                        print()\n",
    "                        results = [step_size, n_slow, m_slow, n_fast, m_fast, cov, np.mean(accuracies), np.std(accuracies), np.max(accuracies), np.min(accuracies), tp, fp, tn, fn, precision, recall, f1_score]\n",
    "                    except:\n",
    "                        print(f\"Model Parameters: {n_slow}, {m_slow}, {n_fast}, {m_fast}, {cov} failed.\")\n",
    "                        results = [step_size, n_slow, m_slow, n_fast, m_fast, cov, None, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "                    results_columns = [\"Feature Type\", \"Window Size\", \"Step Size\", \"Components Slow\", \"Mixtures Slow\",  \"Components Fast\", \"Mixtures Fast\", \"Covariance Type\", \"Mean Accuracy\", \"Standard Deviation\", \"Max Accuracy\", \"Min Accuracy\", \"True Positives\", \"False Positives\", \"True Negatives\", \"False Negatives\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "                    # save the results\n",
    "                    if os.path.exists(\"results-cehmm.csv\"):\n",
    "                        # see if the results file exists\n",
    "                        results_df = pd.read_csv(\"results-cehmm.csv\")\n",
    "                        # add the results to the file\n",
    "                        results_df = pd.concat([results_df, pd.DataFrame([results], columns=results_columns)])\n",
    "                        results_df.to_csv(\"results-cehmm.csv\", index=False)\n",
    "                    else:\n",
    "                        # create the results file\n",
    "                        results_df = pd.DataFrame([results], columns=results_columns)\n",
    "                    results_df.to_csv(\"results-cehmm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
