{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import neurokit2 as nk\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the physiological timestamps / takeover times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_physio_folder_path = \"./Physiological Preprocessed/\"\n",
    "\n",
    "exp2_folder_path = processed_physio_folder_path + \"Exp2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_takeover_times = pd.read_csv(\n",
    "    \"./AdVitam/Exp2/Preprocessed/Physio and Driving/timestamps_obstacles.csv\"\n",
    ")\n",
    "exp2_takeover_times.iloc[:, 2:] = exp2_takeover_times.iloc[:, 2:].apply(pd.to_timedelta, unit=\"s\")\n",
    "exp2_takeover_times[\"subject_id\"] = exp2_takeover_times[\"subject_id\"].apply(\n",
    "    lambda x: x.split(\"T\")[0] + \"T\" + x.split(\"T\")[1].zfill(2)\n",
    ")\n",
    "exp2_takeover_times[\"subject_id\"] = exp2_takeover_times[\"subject_id\"].astype(str)\n",
    "exp2_takeover_times.drop(columns=[\"label_st\"], inplace=True)\n",
    "exp2_takeover_times.sort_values(by=[\"subject_id\"], inplace=True)\n",
    "\n",
    "for column in exp2_takeover_times.columns:\n",
    "    if \"TrigObs\" in column:\n",
    "        exp2_takeover_times = exp2_takeover_times.rename(\n",
    "            columns={column: column.replace(\"TrigObs\", \"\") + \"TOR\"}\n",
    "        )\n",
    "    elif \"RepObs\" in column:\n",
    "        exp2_takeover_times = exp2_takeover_times.rename(\n",
    "            columns={column: column.replace(\"RepObs\", \"Response\")}\n",
    "        )\n",
    "\n",
    "exp2_takeover_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create observations for takoever and driving segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_observations(exp2_folder_path, columns_to_drop, deviation_columns, columns_to_normalize, window_size, step_size):\n",
    "    \"\"\"\n",
    "    Create the observations for the slow and fast takeover times.\n",
    "\n",
    "    Args:\n",
    "        phsyiological_data_dictionary (dict): A dictionary containing the segmented physiological data files.\n",
    "        takeover_times (pd.DataFrame): A DataFrame containing the takeover times.\n",
    "        driver_demographic_data (pd.DataFrame): A DataFrame containing the driver demographic data.\n",
    "        window_length (int): The length of the window in minutes.\n",
    "        window_step (int): The step size for the window\n",
    "        step_sizes (list): A list of step sizes for the window.\n",
    "        tot (str): The threshold for the takeover time.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of observations for the slow takeover times.\n",
    "        list: A list of observations for the fast takeover times.\n",
    "    \"\"\"\n",
    "\n",
    "    driving_observations_data = []\n",
    "    takeover_observations_data = []\n",
    "\n",
    "    # Exp2\n",
    "    phsyiological_data_dictionary = {}\n",
    "    for file in os.listdir(exp2_folder_path):\n",
    "        # Split the file name into the participant and period\n",
    "        f = file.split(\"_\")\n",
    "        participant = f[0]\n",
    "        period = f[1].split(\".\")[0]\n",
    "\n",
    "        if period != \"baseline\" and period != \"driving\":\n",
    "            continue\n",
    "\n",
    "        # Baseline Data\n",
    "        if participant not in phsyiological_data_dictionary:\n",
    "            phsyiological_data_dictionary[participant] = {}\n",
    "            phsyiological_data_dictionary[participant][period] = pd.read_csv(\n",
    "                exp2_folder_path + \"/\" + file\n",
    "            )\n",
    "\n",
    "        # Physiological data\n",
    "        else:\n",
    "            phsyiological_data_dictionary[participant][period] = pd.read_csv(\n",
    "                exp2_folder_path + \"/\" + file\n",
    "            )\n",
    "\n",
    "            # Process the physiological data\n",
    "            baseline_physio = phsyiological_data_dictionary[participant][\"baseline\"].copy()\n",
    "            del phsyiological_data_dictionary[participant][\"baseline\"]\n",
    "            baseline_physio.Time = pd.to_timedelta(baseline_physio.Time)\n",
    "            baseline_physio = baseline_physio.set_index(\"Time\")\n",
    "\n",
    "            experiment_physio = phsyiological_data_dictionary[participant][\"driving\"].copy()\n",
    "            del phsyiological_data_dictionary[participant][\"driving\"]\n",
    "            experiment_physio.Time = pd.to_timedelta(experiment_physio.Time)\n",
    "            experiment_physio = experiment_physio.set_index(\"Time\")\n",
    "\n",
    "            # print(participant)\n",
    "\n",
    "            # Calculate the deviation from the baseline mean\n",
    "            baseline_physio_mean = baseline_physio[deviation_columns].mean()\n",
    "            baseline_deviation = experiment_physio[deviation_columns] - baseline_physio_mean\n",
    "            experiment_physio.drop(\n",
    "                columns=deviation_columns,\n",
    "                inplace=True,\n",
    "            )\n",
    "            experiment_physio = pd.concat([baseline_deviation, experiment_physio], axis=1)\n",
    "            experiment_physio.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "            # calculate the hrv of the baseline\n",
    "            baseline_hrv = nk.hrv(baseline_physio[\"ECG_R_Peaks\"], sampling_rate=1000)\n",
    "\n",
    "            # Obstacle Trigger Times\n",
    "            participant_takeover_times = exp2_takeover_times[\n",
    "                exp2_takeover_times[\"subject_id\"] == participant\n",
    "            ].copy()\n",
    "            participant_takeover_times.iloc[:, 1:] = participant_takeover_times.iloc[:, 1:].apply(\n",
    "                pd.to_timedelta, args=(\"s\",), errors=\"coerce\"\n",
    "            )\n",
    "\n",
    "            participant_obstacle_data = pd.DataFrame()\n",
    "            participant_obstacle_data_length = 0\n",
    "\n",
    "            obstacles = [\"Deer\", \"Cone\", \"Frog\", \"Can\"]\n",
    "            for obstacle in obstacles:\n",
    "                # print(obstacle)\n",
    "                # Participant Takeover Time for the Obstacle\n",
    "\n",
    "                # Obstacle Trigger Time\n",
    "                obstacle_trigger_time = pd.to_timedelta(\n",
    "                    participant_takeover_times[f\"{obstacle}TOR\"].values[0], unit=\"s\"\n",
    "                )\n",
    "                minute_before_obstacle = obstacle_trigger_time - pd.Timedelta(seconds=60)\n",
    "\n",
    "                # If the obstacle trigger time is null, skip the obstacle\n",
    "                if pd.isnull(obstacle_trigger_time):\n",
    "                    continue\n",
    "                if pd.isnull(minute_before_obstacle):\n",
    "                    continue\n",
    "\n",
    "                # Observations 1 minute before and after the obstacle\n",
    "                driving_observations_before_obstacle = experiment_physio.loc[\n",
    "                    minute_before_obstacle - pd.Timedelta(seconds=4) : minute_before_obstacle\n",
    "                ].copy()\n",
    "                driving_observations_after_obstacle = experiment_physio.loc[\n",
    "                    minute_before_obstacle : minute_before_obstacle + pd.Timedelta(seconds=8)\n",
    "                ].copy()\n",
    "\n",
    "                # Observations 3 seconds before and after the obstacle\n",
    "                takeover_observations_before_obstacle = experiment_physio.loc[\n",
    "                    obstacle_trigger_time - pd.Timedelta(seconds=4) : obstacle_trigger_time\n",
    "                ].copy()\n",
    "                takeover_observations_after_obstacle = experiment_physio.loc[\n",
    "                    obstacle_trigger_time : obstacle_trigger_time + pd.Timedelta(seconds=8)\n",
    "                ].copy()\n",
    "\n",
    "                # Debugging statements\n",
    "                # print(f\"Participant: {participant}, Obstacle: {obstacle}\")\n",
    "                # print(f\"Driving observations before obstacle: {len(driving_observations_before_obstacle)}\")\n",
    "                # print(f\"Driving observations after obstacle: {len(driving_observations_after_obstacle)}\")\n",
    "                # print(f\"Takeover observations before obstacle: {len(takeover_observations_before_obstacle)}\")\n",
    "                # print(f\"Takeover observations after obstacle: {len(takeover_observations_after_obstacle)}\")\n",
    "\n",
    "                # Check if the last observation of before obstacle is the same as the first observation of after obstacle\n",
    "                if (\n",
    "                    len(driving_observations_before_obstacle) > 0\n",
    "                    and len(driving_observations_after_obstacle) > 0\n",
    "                    and driving_observations_before_obstacle.tail(1).index\n",
    "                    == driving_observations_after_obstacle.head(1).index\n",
    "                ):\n",
    "                    # drop the first observation of after obstacle\n",
    "                    driving_observations_after_obstacle = driving_observations_after_obstacle.iloc[\n",
    "                        1:\n",
    "                    ]\n",
    "\n",
    "                if (\n",
    "                    len(takeover_observations_before_obstacle) > 0\n",
    "                    and len(takeover_observations_after_obstacle) > 0\n",
    "                    and takeover_observations_before_obstacle.tail(1).index\n",
    "                    == takeover_observations_after_obstacle.head(1).index\n",
    "                ):\n",
    "                    # drop the first observation of after obstacle\n",
    "                    takeover_observations_after_obstacle = (\n",
    "                        takeover_observations_after_obstacle.iloc[1:]\n",
    "                    )\n",
    "                    # print(\"dropped\")\n",
    "                    # print(\"new length\", len(takeover_observations_after_obstacle))\n",
    "\n",
    "                # Check if the length of the observations is 3000\n",
    "                if len(driving_observations_before_obstacle) > 4000:\n",
    "                    # drop the first n rows\n",
    "                    n = len(driving_observations_before_obstacle) - 4000\n",
    "                    driving_observations_before_obstacle = (\n",
    "                        driving_observations_before_obstacle.iloc[n:]\n",
    "                    )\n",
    "                elif len(driving_observations_before_obstacle) < 4000:\n",
    "                    continue\n",
    "\n",
    "                if len(driving_observations_after_obstacle) > 8000:\n",
    "                    # drop the last n rows\n",
    "                    driving_observations_after_obstacle = driving_observations_after_obstacle.iloc[\n",
    "                        :8000\n",
    "                    ]\n",
    "                elif len(driving_observations_after_obstacle) < 8000:\n",
    "                    continue\n",
    "\n",
    "                if len(takeover_observations_before_obstacle) > 4000:\n",
    "                    # drop the first n rows\n",
    "                    n = len(takeover_observations_before_obstacle) - 4000\n",
    "                    takeover_observations_before_obstacle = (\n",
    "                        takeover_observations_before_obstacle.iloc[n:]\n",
    "                    )\n",
    "                elif len(takeover_observations_before_obstacle) < 4000:\n",
    "                    continue\n",
    "\n",
    "                if len(takeover_observations_after_obstacle) > 8000:\n",
    "                    # drop the last n rows\n",
    "                    takeover_observations_after_obstacle = (\n",
    "                        takeover_observations_after_obstacle.iloc[:8000]\n",
    "                    )\n",
    "                elif len(takeover_observations_after_obstacle) < 8000:\n",
    "                    continue\n",
    "\n",
    "                # HRV\n",
    "                driving_hrv_before_obstacle = nk.hrv_time(\n",
    "                    driving_observations_before_obstacle[\"ECG_R_Peaks\"], sampling_rate=1000\n",
    "                )\n",
    "                driving_hrv_after_obstacle = nk.hrv_time(\n",
    "                    driving_observations_after_obstacle[\"ECG_R_Peaks\"], sampling_rate=1000\n",
    "                )\n",
    "\n",
    "                takeover_hrv_before_obstacle = nk.hrv_time(\n",
    "                    takeover_observations_before_obstacle[\"ECG_R_Peaks\"], sampling_rate=1000\n",
    "                )\n",
    "                takeover_hrv_after_obstacle = nk.hrv_time(\n",
    "                    takeover_observations_after_obstacle[\"ECG_R_Peaks\"], sampling_rate=1000\n",
    "                )\n",
    "\n",
    "                if \"ECG_R_Peaks\" in columns_to_drop:\n",
    "                    # drop the ecg_r_peaks from the observations\n",
    "                    driving_observations_before_obstacle.drop(columns=[\"ECG_R_Peaks\"], inplace=True)\n",
    "                    driving_observations_after_obstacle.drop(columns=[\"ECG_R_Peaks\"], inplace=True)\n",
    "\n",
    "                    takeover_observations_before_obstacle.drop(columns=[\"ECG_R_Peaks\"], inplace=True)\n",
    "                    takeover_observations_after_obstacle.drop(columns=[\"ECG_R_Peaks\"], inplace=True)\n",
    "\n",
    "                # add the hrv features to the observations\n",
    "                for column in [\n",
    "                    \"HRV_MeanNN\",\n",
    "                    \"HRV_SDNN\",\n",
    "                    \"HRV_RMSSD\",\n",
    "                    \"HRV_CVSD\",\n",
    "                    \"HRV_MedianNN\",\n",
    "                    \"HRV_MadNN\",\n",
    "                    \"HRV_MCVNN\",\n",
    "                    \"HRV_IQRNN\",\n",
    "                    \"HRV_SDRMSSD\",\n",
    "                    \"HRV_Prc20NN\",\n",
    "                    \"HRV_Prc80NN\",\n",
    "                    \"HRV_pNN50\",\n",
    "                    \"HRV_pNN20\",\n",
    "                    \"HRV_MinNN\",\n",
    "                    \"HRV_MaxNN\",\n",
    "                    \"HRV_HTI\",\n",
    "                    \"HRV_TINN\",\n",
    "                ]:\n",
    "                    driving_observations_before_obstacle = (\n",
    "                        driving_observations_before_obstacle.assign(\n",
    "                            **{\n",
    "                                column: driving_hrv_before_obstacle[column].values[0]\n",
    "                                - baseline_hrv[column].values[0]\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    driving_observations_after_obstacle = (\n",
    "                        driving_observations_after_obstacle.assign(\n",
    "                            **{\n",
    "                                column: driving_hrv_after_obstacle[column].values[0]\n",
    "                                - baseline_hrv[column].values[0]\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    takeover_observations_before_obstacle = (\n",
    "                        takeover_observations_before_obstacle.assign(\n",
    "                            **{\n",
    "                                column: takeover_hrv_before_obstacle[column].values[0]\n",
    "                                - baseline_hrv[column].values[0]\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    takeover_observations_after_obstacle = (\n",
    "                        takeover_observations_after_obstacle.assign(\n",
    "                            **{\n",
    "                                column: takeover_hrv_after_obstacle[column].values[0]\n",
    "                                - baseline_hrv[column].values[0]\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # Combine the observations\n",
    "                driving_observations = pd.concat(\n",
    "                    [driving_observations_before_obstacle, driving_observations_after_obstacle]\n",
    "                )\n",
    "                takeover_observations = pd.concat(\n",
    "                    [takeover_observations_before_obstacle, takeover_observations_after_obstacle]\n",
    "                )\n",
    "\n",
    "                # sliding window with step size\n",
    "                resampled_data = pd.DataFrame()\n",
    "                for column in driving_observations.columns:\n",
    "                    if column in columns_to_normalize:\n",
    "                        resampled_data[column] = (\n",
    "                            driving_observations[column].rolling(window_size).mean()\n",
    "                        )\n",
    "                        resampled_data[column] = resampled_data[column].resample(step_size).mean()\n",
    "                        resampled_data.dropna(inplace=True)\n",
    "                        # resampled_data[column] = resampled_data[column].interpolate(method=\"ffill\")\n",
    "                    else:\n",
    "                        resampled_data[column] = (\n",
    "                            driving_observations[column].rolling(window_size).max()\n",
    "                        )\n",
    "                        resampled_data[column] = resampled_data[column].resample(step_size).max()\n",
    "                        resampled_data.dropna(inplace=True)\n",
    "                        # resampled_data[column] = resampled_data[column].interpolate(method=\"ffill\")\n",
    "                driving_observations = resampled_data\n",
    "\n",
    "                resampled_data = pd.DataFrame()\n",
    "                for column in takeover_observations.columns:\n",
    "                    if column in columns_to_normalize:\n",
    "                        resampled_data[column] = (\n",
    "                            takeover_observations[column].rolling(window_size).mean()\n",
    "                        )\n",
    "                        resampled_data[column] = resampled_data[column].resample(step_size).mean()\n",
    "                        resampled_data.dropna(inplace=True)\n",
    "                        # resampled_data[column] = resampled_data[column].interpolate(method=\"ffill\")\n",
    "                    else:\n",
    "                        resampled_data[column] = (\n",
    "                            takeover_observations[column].rolling(window_size).max()\n",
    "                        )\n",
    "                        resampled_data[column] = resampled_data[column].resample(step_size).max()\n",
    "                        resampled_data.dropna(inplace=True)\n",
    "                        # resampled_data[column] = resampled_data[column].interpolate(method=\"ffill\")\n",
    "                takeover_observations = resampled_data\n",
    "\n",
    "                # add the tot to the observations\n",
    "                driving_observations = driving_observations.assign(takeover=0)\n",
    "                takeover_observations = takeover_observations.assign(takeover=1)\n",
    "\n",
    "                if participant_obstacle_data_length == 0:\n",
    "                    participant_obstacle_data_length = len(driving_observations)\n",
    "\n",
    "                if (\n",
    "                    len(driving_observations)\n",
    "                    != len(takeover_observations)\n",
    "                    != participant_obstacle_data_length\n",
    "                ):\n",
    "                    print(f\"Participant: {participant}, Obstacle: {obstacle}, Lengths do not match\")\n",
    "                    continue\n",
    "\n",
    "                # add the observations to the participant obstacle data\n",
    "                participant_obstacle_data = pd.concat(\n",
    "                    [participant_obstacle_data, driving_observations, takeover_observations]\n",
    "                )\n",
    "\n",
    "            # After looping through the obstacles\n",
    "            # Standardize the data\n",
    "            time = participant_obstacle_data.index\n",
    "            columns = participant_obstacle_data.columns\n",
    "            scaler = StandardScaler()\n",
    "            participant_obstacle_data[columns_to_normalize] = scaler.fit_transform(\n",
    "                participant_obstacle_data[columns_to_normalize]\n",
    "            )\n",
    "            participant_obstacle_data = pd.DataFrame(participant_obstacle_data, columns=columns)\n",
    "            participant_obstacle_data[\"Time\"] = time\n",
    "            participant_obstacle_data = participant_obstacle_data.set_index(\"Time\")\n",
    "\n",
    "            # seperate the data into slow and fast takeovers\n",
    "            while len(participant_obstacle_data) > 0:\n",
    "                # get the first observations\n",
    "                observations = participant_obstacle_data.iloc[:participant_obstacle_data_length]\n",
    "                participant_obstacle_data = participant_obstacle_data.iloc[\n",
    "                    participant_obstacle_data_length:\n",
    "                ]\n",
    "\n",
    "                # takover from the observations\n",
    "                takeover = observations[\"takeover\"].values[0]\n",
    "                observations.drop(columns=[\"takeover\"], inplace=True)\n",
    "\n",
    "                if takeover == 0:\n",
    "                    driving_observations_data.append(observations)\n",
    "                else:\n",
    "                    takeover_observations_data.append(observations)\n",
    "\n",
    "    return driving_observations_data, takeover_observations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy( driving_observations, takeover_observations, n_components_driving, n_components_takeover, n_mix_driving, n_mix_takeover, covariance_type, ):\n",
    "    iterations = 100\n",
    "\n",
    "    accuracies = []\n",
    "    true_positives_list = []\n",
    "    false_positives_list = []\n",
    "    true_negatives_list = []\n",
    "    false_negatives_list = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration: {i}\")\n",
    "\n",
    "        # split the data\n",
    "        driving_observations_train, driving_observations_test = train_test_split(\n",
    "            driving_observations, test_size=0.3\n",
    "        )\n",
    "        takeover_observations_train, takeover_observations_test = train_test_split(\n",
    "            takeover_observations, test_size=0.3\n",
    "        )\n",
    "\n",
    "        # concatenate the observations\n",
    "        X_driving = None\n",
    "        X_driving_lengths = []\n",
    "        for data in driving_observations_train:\n",
    "            if X_driving is None:\n",
    "                X_driving = data.values\n",
    "            else:\n",
    "                X_driving = np.concatenate((X_driving, data.values))\n",
    "            X_driving_lengths.append(len(data))\n",
    "\n",
    "        X_takeover = None\n",
    "        X_takeover_lengths = []\n",
    "        for data in takeover_observations_train:\n",
    "            if X_takeover is None:\n",
    "                X_takeover = data.values\n",
    "            else:\n",
    "                X_takeover = np.concatenate((X_takeover, data.values))\n",
    "            X_takeover_lengths.append(len(data))\n",
    "\n",
    "        # initialize and fit the models\n",
    "        driving_model = hmm.GMMHMM(\n",
    "            n_components=n_components_driving, n_mix=n_mix_driving, covariance_type=covariance_type\n",
    "        )\n",
    "        takeover_model = hmm.GMMHMM(\n",
    "            n_components=n_components_takeover,\n",
    "            n_mix=n_mix_takeover,\n",
    "            covariance_type=covariance_type,\n",
    "        )\n",
    "\n",
    "        # fit the models\n",
    "        driving_model.fit(X_driving, X_driving_lengths)\n",
    "        takeover_model.fit(X_takeover, X_takeover_lengths)\n",
    "\n",
    "        # score the models\n",
    "        accuracy = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "\n",
    "        for _, observation in enumerate(driving_observations_test):\n",
    "            observation = observation.values\n",
    "\n",
    "            if driving_model.score(observation) > takeover_model.score(observation):\n",
    "                accuracy += 1\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "        for _, observation in enumerate(takeover_observations_test):\n",
    "            observation = observation.values\n",
    "\n",
    "            if takeover_model.score(observation) > driving_model.score(observation):\n",
    "                accuracy += 1\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        accuracy = accuracy / (len(driving_observations_test) + len(takeover_observations_test))\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        true_positives_list.append(tp)\n",
    "        false_positives_list.append(fp)\n",
    "        true_negatives_list.append(tn)\n",
    "        false_negatives_list.append(fn)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            print(f\"True Positives: {tp}\")\n",
    "            print(f\"False Positives: {fp}\")\n",
    "            print(f\"True Negatives: {tn}\")\n",
    "            print(f\"False Negatives: {fn}\")\n",
    "\n",
    "    return (\n",
    "        accuracies,\n",
    "        true_positives_list,\n",
    "        false_positives_list,\n",
    "        true_negatives_list,\n",
    "        false_negatives_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Collecting Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating observations into takeover times of greater than or less than 3 seconds, with a sliding window of 3 seconds and a step size of 3 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    [\"CH1\", \"CH2\", \"CH3\", \"ECG_Raw\", \"ECG_Clean\", \"ECG_Quality\", \n",
    "     \"ECG_R_Onsets\", \"ECG_R_Offsets\", \"ECG_P_Peaks\", \"ECG_P_Onsets\", \"ECG_P_Offsets\", \"ECG_Q_Peaks\", \n",
    "     \"ECG_S_Peaks\", \"ECG_T_Peaks\", \"ECG_T_Onsets\", \"ECG_T_Offsets\", \"ECG_Phase_Atrial\", \"ECG_Phase_Completion_Atrial\", \n",
    "     \"ECG_Phase_Ventricular\", \"ECG_Phase_Completion_Ventricular\", \"RSP_Raw\", \"RSP_Clean\", \"RSP_Peaks\", \"RSP_Troughs\", \n",
    "     \"RSP_Amplitude\", \"RSP_Phase\", \"RSP_Phase_Completion\", \"RSP_RVT\", \"EDA_Raw\", \"EDA_Clean\", \n",
    "     \"EDA_Tonic\", \"EDA_Phasic\", \"SCR_Onsets\", \"SCR_Height\", \"SCR_Recovery\", \"RSA_P2T\", \"RSA_Gates\"], # \"Relevant Features\"\n",
    "    [\"CH1\", \"CH2\", \"CH3\", \"ECG_Raw\", \"ECG_Clean\", \"RSP_Raw\", \"RSP_Clean\", \"EDA_Raw\", \"EDA_Clean\"] # \"All Features\"\n",
    "]\n",
    "\n",
    "deviation_columns = [\n",
    "    [\"ECG_Rate\", \"RSP_Rate\"],  # \"Relevant Features\"\n",
    "    [\"ECG_Rate\", \"RSP_Rate\", \"EDA_Rate\"],  # \"All Features\"\n",
    "]\n",
    "\n",
    "columns_to_normalize = [ \n",
    "    [\"ECG_Rate\", \"RSP_Rate\", \"HRV_MeanNN\", \"HRV_SDNN\", \"HRV_RMSSD\", \"HRV_CVSD\", \n",
    "     \"HRV_MedianNN\", \"HRV_MadNN\", \"HRV_MCVNN\", \"HRV_IQRNN\", \"HRV_SDRMSSD\", \"HRV_Prc20NN\", \n",
    "     \"HRV_Prc80NN\", \"HRV_pNN50\", \"HRV_pNN20\", \"HRV_MinNN\", \"HRV_MaxNN\", \"HRV_HTI\", \n",
    "     \"HRV_TINN\"], # \"Relevant Features\"\n",
    "     [\"ECG_Rate\", \"ECG_Quality\", \"ECG_Phase_Completion_Atrial\", \"ECG_Phase_Completion_Ventricular\", \"RSP_Amplitude\", \"RSP_Rate\", \n",
    "      \"RSP_RVT\", \"RSP_Phase_Completion\", \"RSP_Symmetry_PeakTrough\", \"RSP_Symmetry_RiseDecay\", \"EDA_Tonic\", \"RSA_P2T\", \n",
    "      \"RSA_Gates\", \"HRV_MeanNN\", \"HRV_SDNN\", \"HRV_RMSSD\", \"HRV_CVSD\", \"HRV_MedianNN\", \n",
    "      \"HRV_MadNN\", \"HRV_MCVNN\", \"HRV_IQRNN\", \"HRV_SDRMSSD\", \"HRV_Prc20NN\", \"HRV_Prc80NN\", \n",
    "      \"HRV_pNN50\", \"HRV_pNN20\", \"HRV_MinNN\", \"HRV_MaxNN\", \"HRV_HTI\", \"HRV_TINN\"] # \"All Features\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_windows = {\n",
    "    \"3s\": [\"3s\", \"1s\"],\n",
    "    \"1s\": [\"1s\", \"500ms\"],\n",
    "    \"500ms\": [\"500ms\", \"100ms\"],\n",
    "    \"100ms\": [\"100ms\", \"50ms\"],\n",
    "}\n",
    "# model parameters\n",
    "n_components_slow = [1, 2, 3, 4]\n",
    "n_mix_slow = [1, 2, 3, 4]\n",
    "n_components_fast = [1, 2, 3, 4]\n",
    "n_mix_fast = [1, 2, 3, 4]\n",
    "covariance_types = [\"full\", \"diag\", \"spherical\"]\n",
    "\n",
    "for i, columns in enumerate(columns_to_drop):\n",
    "    for window_size in sliding_windows.keys():\n",
    "        for step_size in sliding_windows[window_size]:\n",
    "\n",
    "            # collect the observations for these parameters\n",
    "            feature_type = \"Relevant Features\" if i == 0 else \"All Features\"\n",
    "            print(f\"Collecting {feature_type} with a window size of: {window_size} and a step size of: {step_size}\")\n",
    "            driving_observations, takeover_observations = collect_observations(exp2_folder_path, columns, deviation_columns[i], columns_to_normalize[i], window_size, step_size)\n",
    "            print(f\"Collected {len(driving_observations)} driving observations and {len(takeover_observations)} takeover observations.\")\n",
    "\n",
    "            # iterate over the model parameters\n",
    "            for n_slow in n_components_slow:\n",
    "                for m_slow in n_mix_slow:\n",
    "                    for n_fast in n_components_fast:\n",
    "                        for m_fast in n_mix_fast:\n",
    "                            for cov in covariance_types:\n",
    "\n",
    "                                # check if the results for these parameters already exist\n",
    "                                if os.path.exists(\"results-to-thmm.csv\"):\n",
    "                                    results_df = pd.read_csv(\"results-to-thmm.csv\")\n",
    "                                    results = results_df[\n",
    "                                        (results_df[\"Feature Type\"] == feature_type)\n",
    "                                        & (results_df[\"Window Size\"] == window_size)\n",
    "                                        & (results_df[\"Step Size\"] == step_size)\n",
    "                                        & (results_df[\"Components Slow\"] == n_slow)\n",
    "                                        & (results_df[\"Mixtures Slow\"] == m_slow)\n",
    "                                        & (results_df[\"Components Fast\"] == n_fast)\n",
    "                                        & (results_df[\"Mixtures Fast\"] == m_fast)\n",
    "                                        & (results_df[\"Covariance Type\"] == cov)\n",
    "                                    ]\n",
    "                                    if not results.empty:\n",
    "                                        print(\"Results already exist for these parameters.\")\n",
    "                                        continue\n",
    "\n",
    "                                print(\"-------------------------------------------------\")\n",
    "                                print(f\"Slow Components: {n_slow}, Slow Mixtures: {m_slow}, Fast Components: {n_fast}, Fast Mixtures: {m_fast}, Covariance Type: {cov}\")\n",
    "                                try:\n",
    "                                    accuracies, true_positives_list, false_positives_list, true_negatives_list, false_negatives_list = accuracy(driving_observations, takeover_observations, n_slow, n_fast, m_slow, m_fast, cov)\n",
    "\n",
    "                                    # Accuracy\n",
    "                                    print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
    "                                    print(f\"Standard Deviation: {np.std(accuracies)}\")\n",
    "                                    print(f\"Max Accuracy: {np.max(accuracies)}\")\n",
    "                                    print(f\"Min Accuracy: {np.min(accuracies)}\")\n",
    "\n",
    "                                    # Find the index of the max accuracy\n",
    "                                    max_accuracy_index = accuracies.index(np.max(accuracies))\n",
    "                                    tp = true_positives_list[max_accuracy_index]\n",
    "                                    print(f\"True Positives: {tp}\")\n",
    "                                    fp = false_positives_list[max_accuracy_index]\n",
    "                                    print(f\"False Positives: {fp}\")\n",
    "                                    tn = true_negatives_list[max_accuracy_index]\n",
    "                                    print(f\"True Negatives: {tn}\")\n",
    "                                    fn = false_negatives_list[max_accuracy_index]\n",
    "                                    print(f\"False Negatives: {fn}\")\n",
    "\n",
    "                                    # check  if any of the values are zero\n",
    "                                    if tp + fp == 0 or tp + fn == 0:\n",
    "                                        precision = 0\n",
    "                                        recall = 0\n",
    "                                        f1_score = 0\n",
    "                                    else:\n",
    "                                        # Precision, Recall, and F1 Score\n",
    "                                        precision = tp / (tp + fp)\n",
    "                                        recall = tp / (tp + fn)\n",
    "                                        f1_score = 2 * precision * recall / (precision + recall)\n",
    "                                    print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1_score}')\n",
    "                                    print(\"-------------------------------------------------\")\n",
    "                                    print()\n",
    "                                    results = [feature_type, window_size, step_size, n_slow, m_slow, n_fast, m_fast, cov, np.mean(accuracies), np.std(accuracies), np.max(accuracies), np.min(accuracies), tp, fp, tn, fn, precision, recall, f1_score]\n",
    "                                except:\n",
    "                                    print(f\"Model Parameters: {n_slow}, {m_slow}, {n_fast}, {m_fast}, {cov} failed.\")\n",
    "                                    results = [feature_type, window_size, step_size, n_slow, m_slow, n_fast, m_fast, cov, None, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "                                results_columns = [\"Feature Type\", \"Window Size\", \"Step Size\", \"Components Slow\", \"Mixtures Slow\",  \"Components Fast\", \"Mixtures Fast\", \"Covariance Type\", \"Mean Accuracy\", \"Standard Deviation\", \"Max Accuracy\", \"Min Accuracy\", \"True Positives\", \"False Positives\", \"True Negatives\", \"False Negatives\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "                                # save the results\n",
    "                                if os.path.exists(\"results-to-thmm.csv\"):\n",
    "                                    # see if the results file exists\n",
    "                                    results_df = pd.read_csv(\"results-to-thmm.csv\")\n",
    "                                    # add the results to the file\n",
    "                                    results_df = pd.concat([results_df, pd.DataFrame([results], columns=results_columns)])\n",
    "                                    results_df.to_csv(\"results-to-thmm.csv\", index=False)\n",
    "                                else:\n",
    "                                    # create the results file\n",
    "                                    results_df = pd.DataFrame([results], columns=results_columns)\n",
    "                                results_df.to_csv(\"results-to-thmm.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
